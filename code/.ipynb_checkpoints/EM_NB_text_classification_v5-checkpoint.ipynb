{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update: Study the relation between classification accuracy and quantity of labeled and unlabeled documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import packages and libraries\n",
    "import numpy as np\n",
    "import random as rnd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, ShuffleSplit\n",
    "from sklearn import metrics\n",
    "\n",
    "from time import time\n",
    "from pprint import pprint\n",
    "from copy import deepcopy\n",
    "from wordcloud import WordCloud \n",
    "from Semi_EM_NB import Semi_EM_MultinomialNB\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_noise(sentence):\n",
    "    result = ''\n",
    "    poster = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stopword_set = set(stopwords.words('english'))\n",
    "    wordlist = re.sub(r\"\\n|(\\\\(.*?){)|}|[!$%^&*#()_+|~\\-={}\\[\\]:\\\";'<>?,.\\/\\\\]|[0-9]|[@]\", ' ', sentence) # remove punctuation\n",
    "    wordlist = re.sub('\\s+', ' ', wordlist) # remove extra space\n",
    "    wordlist_normal = [poster.stem(word.lower()) for word in wordlist.split()] # restore word to its original form (stemming)\n",
    "    wordlist_normal = [lemmatizer.lemmatize(word, pos='v') for word in wordlist_normal] # restore word to its root form (lemmatization)\n",
    "    wordlist_clean = [word for word in wordlist_normal if word not in stopword_set] # remove stopwords\n",
    "    result = ' '.join(wordlist_clean)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validation(clf, data_X, data_y, unlabeled=None, n_folds=5):\n",
    "    print('=' * 80)\n",
    "    print(\"Validation: \")\n",
    "    print(clf)\n",
    "    kf = StratifiedKFold(n_splits=n_folds)\n",
    "    start_time = time()\n",
    "    train_accuracies= list() # training accuracy\n",
    "    fold_count = 1\n",
    "    original_clf = deepcopy(clf)\n",
    "    for train_ids, valid_ids in kf.split(data_X, data_y):\n",
    "        cv_clf = deepcopy(original_clf)\n",
    "        print(\"Fold # %d\" % fold_count)\n",
    "        fold_count += 1\n",
    "        train_X, train_y, valid_X, valid_y = data_X[train_ids], data_y[train_ids], data_X[valid_ids], data_y[valid_ids]\n",
    "        if unlabeled==None:\n",
    "            cv_clf.fit(train_X, train_y)\n",
    "        else:\n",
    "            cv_clf.fit(train_X, train_y, unlabeled)\n",
    "        pred = cv_clf.predict(valid_X)\n",
    "        train_accuracies.append(metrics.accuracy_score(valid_y, pred))\n",
    "    train_time = time() - start_time\n",
    "    print(\"Validation time: %0.3f seconds\" % train_time)\n",
    "    print(\"Average training accuracy: %0.3f\" % np.mean(np.array(train_accuracies)))\n",
    "    return train_accuracies, train_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_topK(classifier, vectorizer, categories, K=10):\n",
    "    feature_names = np.asarray(vectorizer.get_feature_names())\n",
    "    fig, axes = plt.subplots(figsize=(50, 40), nrows=5, ncols=4)\n",
    "    for i, category in enumerate(categories):\n",
    "        topK = np.argsort(classifier.coef_[i])[-K:]\n",
    "        text = \" \".join(feature_names[topK])\n",
    "        print(\"%s: %s\" % (category, text))\n",
    "        wordcloud = WordCloud().generate(text)\n",
    "        axes[i//4, i%4].imshow(wordcloud, cmap=plt.cm.gray, interpolation='bilinear')\n",
    "        axes[i//4, i%4].axis(\"off\")\n",
    "        axes[i//4, i%4].set_title(category, fontweight=\"bold\", size=24)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:    15076\tTest set size:     3770\n"
     ]
    }
   ],
   "source": [
    "# Load data set with class labels and split into train and test set\n",
    "test_size_ratio = 0.2\n",
    "data_Xy = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'), shuffle=True)\n",
    "category_names = data_Xy.target_names # text names of all categories\n",
    "train_X, test_X, train_y, test_y = train_test_split(data_Xy.data, data_Xy.target, test_size=test_size_ratio, stratify=data_Xy.target)\n",
    "print(\"Training set size: %8d\\tTest set size: %8d\" % (len(train_X), len(test_X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# preprocess train and test text data\n",
    "train_X_clean = map(remove_noise, train_X)\n",
    "test_X_clean = map(remove_noise, test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15076, 13318) (3770, 13318)\n"
     ]
    }
   ],
   "source": [
    "# Convert all text data into tf-idf vectors \n",
    "vectorizer = TfidfVectorizer(stop_words='english', min_df=5, max_df=0.95)\n",
    "# vectorizer = TfidfVectorizer()\n",
    "train_vec = vectorizer.fit_transform(train_X_clean)\n",
    "test_vec = vectorizer.transform(test_X_clean)\n",
    "print train_vec.shape, test_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1507, 13318) (13569, 13318)\n"
     ]
    }
   ],
   "source": [
    "# Divide train data set into labeled and unlabeled data sets\n",
    "split_ratio = 0.1 # labeled vs total(labeled+unlabeled)\n",
    "X_l, X_u, y_l, y_u = train_test_split(train_vec, train_y, train_size=split_ratio, stratify=train_y)\n",
    "print X_l.shape, X_u.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Validation: \n",
      "MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
      "Fold # 1\n",
      "Fold # 2\n",
      "Fold # 3\n",
      "Fold # 4\n",
      "Fold # 5\n",
      "Validation time: 0.092 seconds\n",
      "Average training accuracy: 0.611\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.6143790849673203,\n",
       "  0.61764705882352944,\n",
       "  0.63815789473684215,\n",
       "  0.58333333333333337,\n",
       "  0.60137457044673537],\n",
       " 0.09226512908935547)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross validation for Naive Bayes classifier \n",
    "# using labeled data set only\n",
    "nb_clf = MultinomialNB(alpha=1e-2)\n",
    "cross_validation(nb_clf, X_l, y_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Validation: \n",
      "MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
      "Fold # 1\n",
      "Initial expected log likelihood = -6083120.203\n",
      "\n",
      "EM iteration #1\n",
      "\tExpected log likelihood = -5599479.783\n",
      "EM iteration #2\n",
      "\tExpected log likelihood = -5592539.888\n",
      "EM iteration #3\n",
      "\tExpected log likelihood = -5591145.943\n",
      "EM iteration #4\n",
      "\tExpected log likelihood = -5590972.413\n",
      "EM iteration #5\n",
      "\tExpected log likelihood = -5590924.138\n",
      "EM iteration #6\n",
      "\tExpected log likelihood = -5590897.762\n",
      "EM iteration #7\n",
      "\tExpected log likelihood = -5590897.762\n",
      "Fold # 2\n",
      "Initial expected log likelihood = -6088705.799\n",
      "\n",
      "EM iteration #1\n",
      "\tExpected log likelihood = -5599145.874\n",
      "EM iteration #2\n",
      "\tExpected log likelihood = -5592642.415\n",
      "EM iteration #3\n",
      "\tExpected log likelihood = -5591526.484\n",
      "EM iteration #4\n",
      "\tExpected log likelihood = -5591201.453\n",
      "EM iteration #5\n",
      "\tExpected log likelihood = -5591069.501\n",
      "EM iteration #6\n",
      "\tExpected log likelihood = -5590995.817\n",
      "EM iteration #7\n",
      "\tExpected log likelihood = -5590948.682\n",
      "EM iteration #8\n",
      "\tExpected log likelihood = -5590927.386\n",
      "EM iteration #9\n",
      "\tExpected log likelihood = -5590926.942\n",
      "EM iteration #10\n",
      "\tExpected log likelihood = -5590918.969\n",
      "EM iteration #11\n",
      "\tExpected log likelihood = -5590918.969"
     ]
    }
   ],
   "source": [
    "# Cross validation for semisupervised EM Naive Bayes classifier \n",
    "# using both labeled and unlabeled data set\n",
    "em_nb_clf = Semi_EM_MultinomialNB(alpha=1e-2) # semi supervised EM based Naive Bayes classifier\n",
    "cross_validation(em_nb_clf, X_l, y_l, X_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluate original NB classifier using test data set\n",
    "nb_clf = MultinomialNB(alpha=1e-2).fit(X_l, y_l)\n",
    "pred = nb_clf.predict(test_vec)\n",
    "print(metrics.classification_report(test_Xy.target, pred, target_names=test_Xy.target_names))\n",
    "# pprint(metrics.confusion_matrix(test_Xy.target, pred))\n",
    "print(metrics.accuracy_score(test_Xy.target, pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
