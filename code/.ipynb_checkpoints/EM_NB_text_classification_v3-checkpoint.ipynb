{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import packages and libraries\n",
    "import numpy as np\n",
    "import random as rnd\n",
    "import nltk as nk\n",
    "import re\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, ShuffleSplit\n",
    "from sklearn import metrics\n",
    "from pprint import pprint\n",
    "\n",
    "from Semi_EM_NB import Semi_EM_MultinomialNB\n",
    "from time import time\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load train and test data set with class labels \n",
    "train_Xy = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "test_Xy = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_noise(sentence):\n",
    "    result = ''\n",
    "    poster = PorterStemmer()\n",
    "    stopword_set = set(stopwords.words('english'))\n",
    "    wordlist = re.sub(r\"\\n|(\\\\(.*?){)|}|[!$%^&*#()_+|~\\-={}\\[\\]:\\\";'<>?,.\\/\\\\]|[0-9]|[@]\", ' ', sentence) # remove punctuation\n",
    "    wordlist = re.sub('\\s+', ' ', wordlist) # remove extra space\n",
    "    wordlist_normal = [poster.stem(word.lower()) for word in wordlist.split()] # restore word to its root form\n",
    "    wordlist_clean = [word for word in wordlist_normal if word not in stopword_set] # remove stopwords\n",
    "    result = ' '.join(wordlist_clean)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# preprocess train and test text data\n",
    "train_Xy.data_clean = map(remove_noise, train_Xy.data)\n",
    "test_Xy.data_clean = map(remove_noise, test_Xy.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 55445) (7532, 55445)\n"
     ]
    }
   ],
   "source": [
    "# Convert all text data into tf-idf vectors \n",
    "# vectorizer = TfidfVectorizer(stop_words='english', min_df=3, max_df=0.9)\n",
    "vectorizer = TfidfVectorizer()\n",
    "train_vec = vectorizer.fit_transform(train_Xy.data_clean)\n",
    "test_vec = vectorizer.transform(test_Xy.data_clean)\n",
    "print train_vec.shape, test_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5657, 55445) (5657, 55445)\n"
     ]
    }
   ],
   "source": [
    "# Divide train data set into labeled and unlabeled data sets\n",
    "n_train_data = train_vec.shape[0]\n",
    "split_ratio = 0.5 # labeled vs unlabeled\n",
    "X_l, X_u, y_l, y_u = train_test_split(train_vec, train_Xy.target, train_size=split_ratio, stratify=train_Xy.target)\n",
    "print X_l.shape, X_u.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validation(clf, data_X, data_y, unlabeled=None, n_folds=5):\n",
    "    print('=' * 80)\n",
    "    print(\"Validation: \")\n",
    "    print(clf)\n",
    "    kf = StratifiedKFold(n_splits=n_folds)\n",
    "    start_time = time()\n",
    "    train_accuracies= list() # training accuracy\n",
    "    fold_count = 1\n",
    "    for train_ids, valid_ids in kf.split(data_X, data_y):\n",
    "        print(\"Fold # %d\" % fold_count)\n",
    "        fold_count += 1\n",
    "        train_X, train_y, valid_X, valid_y = data_X[train_ids], data_y[train_ids], data_X[valid_ids], data_y[valid_ids]\n",
    "        if unlabeled==None:\n",
    "            clf.fit(train_X, train_y)\n",
    "        else:\n",
    "            clf.fit(train_X, train_y, unlabeled)\n",
    "        pred = clf.predict(valid_X)\n",
    "        train_accuracies.append(metrics.accuracy_score(valid_y, pred))\n",
    "    train_time = time() - start_time\n",
    "    print(\"Validation time: %0.3f seconds\" % train_time)\n",
    "    print(\"Average training accuracy: %0.3f\" % np.mean(np.array(train_accuracies)))\n",
    "    return train_accuracies, train_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Validation: \n",
      "MultinomialNB(alpha=1, class_prior=None, fit_prior=True)\n",
      "Fold # 1\n",
      "Fold # 2\n",
      "Fold # 3\n",
      "Fold # 4\n",
      "Fold # 5\n",
      "Validation time: 0.342 seconds\n",
      "Average training accuracy: 0.663\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.66929824561403506,\n",
       "  0.66402814423922607,\n",
       "  0.66843501326259946,\n",
       "  0.64742451154529312,\n",
       "  0.66429207479964381],\n",
       " 0.34151387214660645)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross validation for Naive Bayes classifier \n",
    "# using labeled data set only\n",
    "nb_clf = MultinomialNB(alpha=1)\n",
    "cross_validation(nb_clf, X_l, y_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Validation: \n",
      "MultinomialNB(alpha=1, class_prior=None, fit_prior=True)\n",
      "Fold # 1\n",
      "Initial expected log likelihood = -3820685.485\n",
      "\n",
      "EM iteration #1\n",
      "\tExpected log likelihood = -3626889.086\n",
      "EM iteration #2\n",
      "\tExpected log likelihood = -3457240.369\n",
      "EM iteration #3\n",
      "\tExpected log likelihood = -3339599.120\n",
      "EM iteration #4\n",
      "\tExpected log likelihood = -3331276.741\n",
      "EM iteration #5\n",
      "\tExpected log likelihood = -3331211.154\n",
      "EM iteration #6\n",
      "\tExpected log likelihood = -3331211.154\n",
      "Fold # 2\n",
      "Initial expected log likelihood = -3820368.239\n",
      "\n",
      "Fold # 3\n",
      "Initial expected log likelihood = -3819610.726\n",
      "\n",
      "EM iteration #1\n",
      "\tExpected log likelihood = -3628008.032\n",
      "EM iteration #2\n",
      "\tExpected log likelihood = -3462848.680\n",
      "EM iteration #3\n",
      "\tExpected log likelihood = -3342948.738\n",
      "EM iteration #4\n",
      "\tExpected log likelihood = -3331052.865\n",
      "EM iteration #5\n",
      "\tExpected log likelihood = -3331122.336\n",
      "Fold # 4\n",
      "Initial expected log likelihood = -3819945.654\n",
      "\n",
      "Fold # 5\n",
      "Initial expected log likelihood = -3821841.407\n",
      "\n",
      "Validation time: 114.253 seconds\n",
      "Average training accuracy: 0.416\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.052631578947368418,\n",
       "  0.66402814423922607,\n",
       "  0.053050397877984087,\n",
       "  0.64742451154529312,\n",
       "  0.66429207479964381],\n",
       " 114.25321817398071)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross validation for semisupervised EM Naive Bayes classifier \n",
    "# using both labeled and unlabeled data set\n",
    "em_nb_clf = Semi_EM_MultinomialNB(alpha=1) # semi supervised EM based Naive Bayes classifier\n",
    "cross_validation(em_nb_clf, X_l, y_l, X_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
