import numpy as npfrom copy import deepcopyfrom scipy.sparse import csr_matrixfrom sklearn.naive_bayes import MultinomialNBfrom sklearn.naive_bayes import GaussianNBclass Semi_EM_MultinomialNB():    """    Naive Bayes classifier for multinomial models for semi-supervised learning.        Use both labeled and unlabeled data to train NB classifier, update parameters    using unlabeled data, and all data to evaluate performance of classifier. Optimize    classifier using Expectation-Maximization algorithm.    """    def __init__(self, alpha=1.0, fit_prior=True, class_prior=None, max_iter=5, tol=1e-6):        self.alpha = alpha        self.fit_prior = fit_prior        self.class_prior = class_prior        self.clf = MultinomialNB(alpha=self.alpha, fit_prior=self.fit_prior, class_prior=self.class_prior)        self.log_lkh = 0 # log likelihood        self.max_iter = max_iter # max number of EM iterations        self.tol = tol # tolerance of log likelihood increment    def fit(self, X_l, y_l, X_u):        n_ul_docs = X_u.shape[0] # number of unlabeled samples        # initialization (n_docs = n_ul_docs)        clf = deepcopy(self.clf)# build new copy of classifier        clf.fit(X_l, y_l) # use labeled data only to initialize classifier parameters        prev_log_lkh = self.log_lkh # record log likelihood of previous EM iteration        lp_w_c = csr_matrix(clf.feature_log_prob_) # log CP of word given class [n_classes, n_words]        b_w_d = csr_matrix(X_u > 0, dtype=int) # words in each document        lp_d_c = lp_w_c.dot(b_w_d.transpose()) # log CP of doc given class [n_classes, n_docs]        lp_c = np.matrix(clf.class_log_prior_).T # log prob of classes [n_classes, 1]        lp_c = csr_matrix(np.repeat(lp_c, n_ul_docs, axis=1)) # repeat for each doc [n_classes, n_docs]        lp_dc = lp_d_c + lp_c # joint prob of doc and class [n_classes, n_docs]        p_c_d = clf.predict_proba(X_u) # weight of each class in each doc [n_docs, n_classes]        expectation = (p_c_d.dot(lp_dc)).trace() # expectation of log likelihood over all unlabeled docs        self.clf = deepcopy(clf)        self.log_lkh = expectation        # Loop until log likelihood does not improve        iter_count = 0 # count EM iteration        while (self.log_lkh-prev_log_lkh>=self.tol and iter_count<self.max_iter):            iter_count += 1            # E-step: Estimate class membership of unlabeled documents            y_u = clf.predict(X_u)            # M-step: Re-estimate classifier parameters            X = np.concatenate((X_l, X_u), axis=0)            y = np.concatenate((y_l, y_u), axis=0)            clf.fit(X, y)            # check convergence: update log likelihood            p_c_d = clf.predict_proba(X_u)            lp_w_c = csr_matrix(clf.feature_log_prob_)  # log CP of word given class [n_classes, n_words]            b_w_d = csr_matrix(X_u > 0, dtype=int)  # words in each document            lp_d_c = lp_w_c.dot(b_w_d.transpose())  # log CP of doc given class [n_classes, n_docs]            lp_c = clf.class_log_prior_  # log prob of classes [n_classes, 1]            lp_c = csr_matrix(np.repeat(lp_c, n_ul_docs, axis=1))  # repeat for each doc [n_classes, n_docs]            lp_dc = lp_d_c + lp_c  # joint prob of doc and class [n_classes, n_docs]            expectation = (p_c_d.dot(lp_dc)).trace() # expectation of log likelihood over all unlabeled docs            if (expectation-self.log_lkh >= self.tol):                prev_log_lkh = self.log_lkh                self.log_lkh = expectation                self.clf = deepcopy(clf)            else:                break        return self    def partial_fit(self, X_l, y_l, X_u):        pass    def predict(self, X):        pass    def score(self, X, y):        pass